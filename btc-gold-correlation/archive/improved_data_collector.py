"""
æ”¹è¿›çš„æ•°æ®æ”¶é›†è„šæœ¬ - ä½¿ç”¨æ›´å¯é çš„æ•°æ®æº
åŸºäºGeminiä¸“å®¶åé¦ˆï¼Œè§£å†³åŸæ–¹æ¡ˆçš„è‡´å‘½ç¼ºé™·

æ•°æ®æºå‡çº§ï¼š
- BTC: Binanceäº¤æ˜“æ‰€åŸå§‹æ•°æ®ï¼ˆCCXTåº“ï¼‰
- é»„é‡‘: XAU/USDç°è´§ï¼ˆAlpha Vantage APIï¼‰
- DXY: FREDå®˜æ–¹æ•°æ®
- SPX: FREDå®˜æ–¹æ•°æ®

å…³é”®æ”¹è¿›ï¼š
1. ä¸ä½¿ç”¨forward fillå¡«å……å‘¨æœ«æ•°æ®
2. ä¿ç•™NaNï¼Œè®©pandas.corr()è‡ªåŠ¨å¤„ç†
3. åªåœ¨ä¸¤è€…éƒ½äº¤æ˜“çš„æ—¥å­è®¡ç®—ç›¸å…³æ€§
"""

import ccxt
import pandas as pd
import numpy as np
from alpha_vantage.foreignexchange import ForeignExchange
from pandas_datareader import data as pdr
import time
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Alpha Vantage APIå¯†é’¥
ALPHA_VANTAGE_KEY = '11A6UEZO56SX8FC9'


def fetch_btc_data(start_date='2015-01-01', end_date=None):
    """
    ä»Binanceè·å–BTC/USDTæ—¥çº¿æ•°æ®

    ä¼˜ç‚¹ï¼š
    - äº¤æ˜“æ‰€ä¸€æ‰‹æ•°æ®ï¼Œè´¨é‡é«˜
    - 24/7äº¤æ˜“ï¼ŒçœŸå®åæ˜ å¸‚åœº
    - UTC 00:00å¯¹é½
    """
    print("ğŸ“ˆ æ­£åœ¨ä»Binanceè·å–BTCæ•°æ®...")

    try:
        exchange = ccxt.binance({
            'enableRateLimit': True,
            'options': {'defaultType': 'spot'}
        })

        # å°†æ—¥æœŸè½¬æ¢ä¸ºæ¯«ç§’æ—¶é—´æˆ³
        since = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp() * 1000)

        # è·å–æ‰€æœ‰å†å²æ•°æ®
        all_ohlcv = []
        limit = 1000  # Binanceä¸€æ¬¡æœ€å¤šè¿”å›1000æ¡

        while True:
            ohlcv = exchange.fetch_ohlcv('BTC/USDT', '1d', since=since, limit=limit)

            if not ohlcv:
                break

            all_ohlcv.extend(ohlcv)

            # æ›´æ–°sinceä¸ºæœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´æˆ³+1å¤©
            since = ohlcv[-1][0] + 86400000

            # å¦‚æœæŒ‡å®šäº†ç»“æŸæ—¥æœŸ
            if end_date:
                end_timestamp = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp() * 1000)
                if since >= end_timestamp:
                    break

            # å¦‚æœè·å–çš„æ•°æ®å°‘äºlimitï¼Œè¯´æ˜å·²ç»åˆ°æœ€æ–°æ•°æ®
            if len(ohlcv) < limit:
                break

            time.sleep(exchange.rateLimit / 1000)  # éµå®ˆé€Ÿç‡é™åˆ¶

            # è¿›åº¦æç¤º
            last_date = datetime.fromtimestamp(ohlcv[-1][0] / 1000).strftime('%Y-%m-%d')
            print(f"  å·²è·å–åˆ°: {last_date}", end='\r')

        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['date'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('date', inplace=True)

        print(f"\nâœ… BTCæ•°æ®è·å–å®Œæˆ: {len(df)} æ¡è®°å½• ({df.index[0].date()} è‡³ {df.index[-1].date()})")

        return df[['close']].rename(columns={'close': 'BTC'})

    except Exception as e:
        print(f"âŒ BTCæ•°æ®è·å–å¤±è´¥: {e}")
        print("âš ï¸  å›é€€åˆ°yfinanceå¤‡ç”¨æ–¹æ¡ˆ...")

        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨yfinance
        import yfinance as yf
        btc = yf.download('BTC-USD', start=start_date, end=end_date, progress=False)
        return btc[['Close']].rename(columns={'Close': 'BTC'})


def fetch_gold_data_alphavantage(api_key, start_date='2015-01-01'):
    """
    ä»Alpha Vantageè·å–XAU/USDç°è´§æ—¥çº¿æ•°æ®

    ä¼˜ç‚¹ï¼š
    - çœŸå®çš„ç°è´§é»„é‡‘ä»·æ ¼ï¼ˆè€ŒéETFæˆ–æœŸè´§ï¼‰
    - 24/5äº¤æ˜“ï¼Œä¸BTCæ—¶æ®µæœ€æ¥è¿‘
    - å…è´¹APIï¼Œè´¨é‡å¯é 
    """
    print("ğŸ¥‡ æ­£åœ¨ä»Alpha Vantageè·å–é»„é‡‘æ•°æ®...")

    try:
        # ä½¿ç”¨ç›´æ¥çš„requestsè°ƒç”¨ï¼Œå› ä¸ºalpha_vantageåº“å¯èƒ½æœ‰é—®é¢˜
        import requests

        url = f'https://www.alphavantage.co/query?function=FX_DAILY&from_symbol=XAU&to_symbol=USD&outputsize=full&apikey={api_key}'

        response = requests.get(url)
        data = response.json()

        # æ£€æŸ¥é”™è¯¯
        if 'Error Message' in data:
            raise Exception(data['Error Message'])
        if 'Note' in data:
            raise Exception(data['Note'])
        if 'Time Series FX (Daily)' not in data:
            raise Exception(f"Unexpected response: {list(data.keys())}")

        # è§£ææ•°æ®
        time_series = data['Time Series FX (Daily)']

        # è½¬æ¢ä¸ºDataFrame
        df_data = []
        for date_str, values in time_series.items():
            df_data.append({
                'date': pd.to_datetime(date_str),
                'close': float(values['4. close'])
            })

        df = pd.DataFrame(df_data)
        df = df.sort_values('date')
        df.set_index('date', inplace=True)
        df = df[df.index >= start_date]

        gold = df['close'].rename('Gold')

        print(f"âœ… é»„é‡‘æ•°æ®è·å–å®Œæˆ: {len(gold)} æ¡è®°å½• ({gold.index[0].date()} è‡³ {gold.index[-1].date()})")

        return pd.DataFrame(gold)

    except Exception as e:
        print(f"âŒ Alpha Vantageè·å–å¤±è´¥: {e}")
        print("âš ï¸  å°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
        return None


def fetch_gold_data_yfinance_backup(start_date='2015-01-01', end_date=None):
    """
    å¤‡ç”¨æ–¹æ¡ˆï¼šä»yfinanceè·å–GLD ETFæ•°æ®
    æ³¨æ„ï¼šè¿™ä¸æ˜¯æœ€ä½³æ–¹æ¡ˆï¼Œä½†ä½œä¸ºå¤‡ä»½
    """
    print("ğŸ¥‡ ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: yfinance GLD ETF...")

    import yfinance as yf
    gold = yf.download('GLD', start=start_date, end=end_date, progress=False)

    if not gold.empty:
        print(f"âœ… GLDæ•°æ®è·å–å®Œæˆ: {len(gold)} æ¡è®°å½•")

        # å¤„ç†å¯èƒ½çš„å¤šçº§åˆ—ç´¢å¼•
        if isinstance(gold.columns, pd.MultiIndex):
            gold.columns = gold.columns.droplevel(1)

        # ç¡®ä¿ç´¢å¼•æ˜¯DatetimeIndex
        if not isinstance(gold.index, pd.DatetimeIndex):
            gold.index = pd.to_datetime(gold.index)

        # æ£€æŸ¥åˆ—å
        if 'Close' in gold.columns:
            return gold[['Close']].rename(columns={'Close': 'Gold'})
        else:
            # ä½¿ç”¨ç¬¬ä¸€åˆ—ï¼ˆé€šå¸¸æ˜¯æ”¶ç›˜ä»·ï¼‰
            return pd.DataFrame({' Gold': gold.iloc[:, 3]})  # ç¬¬4åˆ—é€šå¸¸æ˜¯Close
    else:
        print("âŒ GLDæ•°æ®è·å–å¤±è´¥")
        return None


def fetch_dxy_data(start_date='2015-01-01', end_date=None):
    """
    ä»FREDè·å–ç¾å…ƒæŒ‡æ•°

    ä¼˜ç‚¹ï¼š
    - å®˜æ–¹æƒå¨æ•°æ®
    - å…è´¹ã€ç¨³å®š
    - è´¨é‡æé«˜
    """
    print("ğŸ’µ æ­£åœ¨ä»FREDè·å–ç¾å…ƒæŒ‡æ•°...")

    try:
        dxy = pdr.DataReader('DTWEXBGS', 'fred', start_date, end_date)

        print(f"âœ… DXYæ•°æ®è·å–å®Œæˆ: {len(dxy)} æ¡è®°å½•")

        return dxy.rename(columns={'DTWEXBGS': 'DXY'})

    except Exception as e:
        print(f"âŒ DXYæ•°æ®è·å–å¤±è´¥: {e}")
        return None


def fetch_spx_data(start_date='2015-01-01', end_date=None):
    """
    ä»FREDè·å–S&P 500æŒ‡æ•°
    """
    print("ğŸ“Š æ­£åœ¨ä»FREDè·å–S&P 500...")

    try:
        spx = pdr.DataReader('SP500', 'fred', start_date, end_date)

        print(f"âœ… SPXæ•°æ®è·å–å®Œæˆ: {len(spx)} æ¡è®°å½•")

        return spx.rename(columns={'SP500': 'SPX'})

    except Exception as e:
        print(f"âŒ SPXæ•°æ®è·å–å¤±è´¥: {e}")
        return None


def align_data_correctly(btc, gold, dxy=None, spx=None):
    """
    æ­£ç¡®çš„æ•°æ®å¯¹é½æ–¹æ¡ˆ

    å…³é”®æ”¹è¿›ï¼š
    1. ä¸ä½¿ç”¨forward fill
    2. ä¿ç•™NaN
    3. pandas.corr()ä¼šè‡ªåŠ¨å¿½ç•¥NaNé…å¯¹

    è¿™æ ·è®¡ç®—ç›¸å…³æ€§æ—¶ï¼Œåªåœ¨ä¸¤è€…éƒ½äº¤æ˜“çš„æ—¥å­è¿›è¡Œè®¡ç®—
    """
    print("\nğŸ”„ æ­£åœ¨å¯¹é½æ•°æ®...")

    # å¤–è¿æ¥åˆå¹¶ï¼ˆä¿ç•™æ‰€æœ‰æ—¥æœŸï¼‰
    # ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰Serieséƒ½æ˜¯1ç»´çš„
    df = btc.copy()

    if gold is not None:
        if isinstance(gold, pd.DataFrame):
            df = df.join(gold, how='outer')
        else:
            df['Gold'] = gold

    if dxy is not None:
        if isinstance(dxy, pd.DataFrame):
            df = df.join(dxy, how='outer')
        else:
            df['DXY'] = dxy

    if spx is not None:
        if isinstance(spx, pd.DataFrame):
            df = df.join(spx, how='outer')
        else:
            df['SPX'] = spx

    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“… æ•°æ®èŒƒå›´: {df.index[0].date()} è‡³ {df.index[-1].date()}")
    print(f"ğŸ“Š æ€»å¤©æ•°: {len(df)}")
    print(f"\nå„èµ„äº§æ•°æ®ç‚¹æ•°é‡:")
    for col in df.columns:
        valid_count = df[col].notna().sum()
        coverage = valid_count / len(df) * 100
        print(f"  {col}: {valid_count} ({coverage:.1f}%)")

    # æ£€æŸ¥å‘¨æœ«æ•°æ®ï¼ˆå…³é”®éªŒè¯ï¼‰
    weekend_data = df[df.index.dayofweek >= 5]

    print(f"\nğŸ” å‘¨æœ«æ•°æ®æ£€æŸ¥ (å…±{len(weekend_data)}ä¸ªå‘¨æœ«æ—¥):")
    for col in df.columns:
        weekend_count = weekend_data[col].notna().sum()
        print(f"  {col}: {weekend_count} ä¸ªæœ‰æ•ˆç‚¹", end='')

        if col == 'BTC':
            print(" (æ­£å¸¸ï¼ŒBTC 24/7äº¤æ˜“)")
        elif weekend_count > len(weekend_data) * 0.1:
            print(" âš ï¸  å¼‚å¸¸ï¼å‘¨æœ«ä¸åº”è¯¥æœ‰è¿™ä¹ˆå¤šæ•°æ®")
        else:
            print(" âœ… (æ­£å¸¸ï¼Œå‘¨æœ«ä¼‘å¸‚)")

    return df


def calculate_returns(df):
    """
    è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡

    æ³¨æ„ï¼šä¸å¡«å……NaNï¼
    """
    print("\nğŸ“ˆ è®¡ç®—æ”¶ç›Šç‡...")

    returns = np.log(df / df.shift(1))

    # ç»Ÿè®¡
    print(f"\næ”¶ç›Šç‡ç»Ÿè®¡:")
    for col in returns.columns:
        valid = returns[col].dropna()
        print(f"  {col}: {len(valid)} ä¸ªæœ‰æ•ˆæ”¶ç›Šç‡ç‚¹")

    return returns


def calculate_correlation(returns, window=40):
    """
    è®¡ç®—æ»šåŠ¨ç›¸å…³æ€§

    pandasçš„rolling.corr()ä¼šè‡ªåŠ¨å¿½ç•¥é…å¯¹ä¸­çš„NaN
    """
    print(f"\nğŸ”— è®¡ç®—{window}å¤©æ»šåŠ¨ç›¸å…³æ€§...")

    if 'Gold' not in returns.columns:
        print("âŒ ç¼ºå°‘é»„é‡‘æ•°æ®ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")
        return None

    correlation = returns['BTC'].rolling(window).corr(returns['Gold'])

    # è®¡ç®—æœ‰æ•ˆçª—å£å¤§å°
    both_valid = returns[['BTC', 'Gold']].notna().all(axis=1)
    valid_pairs = both_valid.rolling(window).sum()

    avg_valid = valid_pairs.mean()
    min_valid = valid_pairs.min()

    print(f"âœ… ç›¸å…³æ€§è®¡ç®—å®Œæˆ")
    print(f"  å¹³å‡æœ‰æ•ˆé…å¯¹æ•°: {avg_valid:.1f}/{window} ({avg_valid/window*100:.1f}%)")
    print(f"  æœ€å°æœ‰æ•ˆé…å¯¹æ•°: {min_valid:.0f}/{window}")

    # è­¦å‘Šï¼šå¦‚æœæœ‰æ•ˆé…å¯¹æ•°å¤ªå°‘
    if avg_valid < window * 0.8:
        print(f"âš ï¸  è­¦å‘Šï¼šå¹³å‡æœ‰æ•ˆé…å¯¹æ•°ä½äº80%ï¼Œç›¸å…³æ€§å¯èƒ½ä¸å¤Ÿç¨³å¥")

    return correlation, valid_pairs


def validate_data_quality(df, returns):
    """
    æ•°æ®è´¨é‡éªŒè¯
    """
    print("\n" + "="*60)
    print("ğŸ“‹ æ•°æ®è´¨é‡éªŒè¯")
    print("="*60)

    # 1. å¹³ç¨³æ€§æ£€éªŒï¼ˆADFæ£€éªŒï¼‰
    print("\n1ï¸âƒ£  å¹³ç¨³æ€§æ£€éªŒ (ADF Test):")

    try:
        from statsmodels.tsa.stattools import adfuller

        for col in returns.columns:
            result = adfuller(returns[col].dropna())
            p_value = result[1]
            is_stationary = "âœ… å¹³ç¨³" if p_value < 0.05 else "âŒ éå¹³ç¨³"
            print(f"  {col}: p={p_value:.4f} {is_stationary}")

    except ImportError:
        print("  âš ï¸  statsmodelsæœªå®‰è£…ï¼Œè·³è¿‡ADFæ£€éªŒ")
        print("  å®‰è£…å‘½ä»¤: pip install statsmodels")

    # 2. åŸºæœ¬ç»Ÿè®¡
    print("\n2ï¸âƒ£  æ”¶ç›Šç‡åŸºæœ¬ç»Ÿè®¡:")
    print(returns.describe())

    # 3. ç¼ºå¤±å€¼æ¨¡å¼
    print("\n3ï¸âƒ£  ç¼ºå¤±å€¼æ¨¡å¼åˆ†æ:")

    # æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­å¤§é‡ç¼ºå¤±
    for col in df.columns:
        missing = df[col].isna()
        if missing.any():
            # æ‰¾å‡ºæœ€é•¿çš„è¿ç»­ç¼ºå¤±
            groups = (missing != missing.shift()).cumsum()
            max_consecutive = missing.groupby(groups).sum().max()
            print(f"  {col}: æœ€é•¿è¿ç»­ç¼ºå¤± {max_consecutive} å¤©")
        else:
            print(f"  {col}: æ— ç¼ºå¤±å€¼")

    print("\n" + "="*60)


def save_data(df, returns, correlation, valid_pairs, filename_base='improved_data'):
    """
    ä¿å­˜æ•°æ®åˆ°Parquetæ ¼å¼
    """
    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®åˆ°Parquet...")

    # ä¿å­˜åŸå§‹ä»·æ ¼
    price_file = f'{filename_base}_prices.parquet'
    df.to_parquet(price_file)
    print(f"  âœ… ä»·æ ¼æ•°æ®: {price_file}")

    # ä¿å­˜æ”¶ç›Šç‡
    returns_file = f'{filename_base}_returns.parquet'
    returns.to_parquet(returns_file)
    print(f"  âœ… æ”¶ç›Šç‡æ•°æ®: {returns_file}")

    # ä¿å­˜ç›¸å…³æ€§
    if correlation is not None:
        corr_df = pd.DataFrame({
            'correlation': correlation,
            'valid_pairs': valid_pairs
        })
        corr_file = f'{filename_base}_correlation.parquet'
        corr_df.to_parquet(corr_file)
        print(f"  âœ… ç›¸å…³æ€§æ•°æ®: {corr_file}")

    print(f"\nâœ… æ‰€æœ‰æ•°æ®å·²ä¿å­˜")


def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„æ•°æ®æ”¶é›†æµç¨‹
    """
    print("="*60)
    print("ğŸš€ æ”¹è¿›çš„æ•°æ®æ”¶é›†è„šæœ¬")
    print("="*60)
    print(f"\nå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # é…ç½®
    START_DATE = '2015-01-01'
    END_DATE = None  # Noneè¡¨ç¤ºåˆ°ä»Šå¤©

    # 1. è·å–BTCæ•°æ®
    btc = fetch_btc_data(START_DATE, END_DATE)

    # 2. è·å–é»„é‡‘æ•°æ®
    gold = fetch_gold_data_alphavantage(ALPHA_VANTAGE_KEY, START_DATE)

    # å¦‚æœAlpha Vantageå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
    if gold is None or gold.empty:
        gold = fetch_gold_data_yfinance_backup(START_DATE, END_DATE)

    # 3. è·å–è¾…åŠ©æ•°æ®
    dxy = fetch_dxy_data(START_DATE, END_DATE)
    spx = fetch_spx_data(START_DATE, END_DATE)

    # 4. å¯¹é½æ•°æ®ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
    df = align_data_correctly(btc, gold, dxy, spx)

    # 5. è®¡ç®—æ”¶ç›Šç‡
    returns = calculate_returns(df)

    # 6. è®¡ç®—ç›¸å…³æ€§
    correlation, valid_pairs = calculate_correlation(returns, window=40)

    # 7. æ•°æ®è´¨é‡éªŒè¯
    validate_data_quality(df, returns)

    # 8. ä¿å­˜æ•°æ®
    save_data(df, returns, correlation, valid_pairs)

    print("\n" + "="*60)
    print(f"âœ… æ•°æ®æ”¶é›†å®Œæˆï¼")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    return df, returns, correlation


if __name__ == '__main__':
    df, returns, correlation = main()
